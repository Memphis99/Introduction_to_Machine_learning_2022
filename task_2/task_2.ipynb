{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import feature_selection\n",
    "from joblib import dump, load\n",
    "\n",
    "pid = pd.read_csv('test_features.csv').iloc[:,0:1] # extract pid for submission\n",
    "pid = pid.values.tolist()\n",
    "pid = np.array(pid[0::12])\n",
    "header = pd.read_csv('sample.csv', header = None).loc[0,:] # extract test names for submission\n",
    "header = list(header)\n",
    "\n",
    "train_features = pd.read_csv('train_features.csv').iloc[:, 1:] # skip pids\n",
    "test_features = pd.read_csv('test_features.csv').iloc[:, 1:] # skip pids\n",
    "\n",
    "train_labels_1 = pd.read_csv('train_labels.csv').loc[:,'LABEL_BaseExcess':'LABEL_Sepsis']\n",
    "train_labels_3 = pd.read_csv('train_labels.csv').loc[:,'LABEL_RRate':'LABEL_Heartrate']\n",
    "\n",
    "#subtak 1\n",
    "Svm = False\n",
    "Mlp = False\n",
    "GradientBoostingClassification = True\n",
    "\n",
    "#subtask 3\n",
    "normalridge = False\n",
    "kernelridge = False #(peggiore) lol\n",
    "GradientBoostingRegressor = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PRE PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_up(X_nan):\n",
    "    i = 0\n",
    "    for col in X_nan.transpose():\n",
    "        i += 1\n",
    "        num_non_nan = np.count_nonzero(~np.isnan(col))\n",
    "        if num_non_nan/col.size < 0.99:\n",
    "            print('colonna ', i, 'ha meno dell 99% di dati significativi')\n",
    "            print('Dimensioni della matrice:', X_nan.shape)\n",
    "            X_nan = np.delete(X_nan, i, 1)\n",
    "    return X_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sk_imputation(X_nan, method):\n",
    "    num_pazienti = int(X_nan.shape[0]/12)\n",
    "    X = np.empty((num_pazienti, X_nan.shape[1]))\n",
    "\n",
    "    for rows in np.arange(0,X_nan.shape[0],12):\n",
    "        X[int(rows/12),:] = np.nanmean(X_nan[rows:rows+11, :], axis=0)\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy=method, fill_value=0)\n",
    "    X = imputer.fit_transform(X)\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sk_imputation_iterative(X_nan):\n",
    "    num_pazienti = int(X_nan.shape[0]/12)\n",
    "    X = np.empty((num_pazienti, X_nan.shape[1]*2))\n",
    "    for rows in np.arange(0,X_nan.shape[0],12):\n",
    "        X[int(rows/12),:] = np.array(list(zip(np.nanmean(X_nan[int(rows):int(rows+11), :],axis=0),np.nanvar(X_nan[int(rows):int(rows+11), :],axis=0)))).flatten()\n",
    "        #X[int(rows/12),:] = np.nanmean(X_nan[rows:rows+11, :], axis=0)\n",
    "    imp_mean = IterativeImputer(random_state=0, max_iter = 100, initial_strategy='most_frequent', verbose = 2)\n",
    "    imp_mean.fit(X)\n",
    "    X = imp_mean.transform(X)\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validation(X_train, y_label, do_validation = False):\n",
    "    # function to split X_train and y_label for validation and hyperparameter tuning\n",
    "    if do_validation:\n",
    "        nvalid = 0.2 # 0 means no validation set (ratio!), 0.2 = 20% on validation and 80% on training\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(X_train, y_label, test_size=nvalid, random_state=12345)\n",
    "    else:\n",
    "        x_train = X_train\n",
    "        y_train = y_label\n",
    "\n",
    "    return(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s4/bm5cfwp97214173jb5bgzcm40000gn/T/ipykernel_4945/2005761605.py:6: RuntimeWarning: Mean of empty slice\n",
      "  X[int(rows/12),:] = np.nanmean(X_nan[rows:rows+11, :], axis=0)\n",
      "/var/folders/s4/bm5cfwp97214173jb5bgzcm40000gn/T/ipykernel_4945/2005761605.py:6: RuntimeWarning: Mean of empty slice\n",
      "  X[int(rows/12),:] = np.nanmean(X_nan[rows:rows+11, :], axis=0)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train_features)\n",
    "X_test = np.array(test_features)\n",
    "\n",
    "Simple = True\n",
    "if Simple:\n",
    "    X_train = sk_imputation(X_train, 'constant')\n",
    "    X_test = sk_imputation(X_test, 'constant')\n",
    "else:\n",
    "    X_train = sk_imputation_iterative(X_train)\n",
    "    X_test = sk_imputation_iterative(X_test)\n",
    "\n",
    "NormalizeData = True\n",
    "if NormalizeData :\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = preprocessing.MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SUB-TASK 1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot_medical_test 11\n"
     ]
    }
   ],
   "source": [
    "# cell to visualize correletion, uncomment below\n",
    "x_train, y_train = X_train, np.array(train_labels_1)\n",
    "tot_medical_test = np.array(train_labels_1).shape[1]\n",
    "print(\"tot_medical_test\",tot_medical_test)\n",
    "df = pd.DataFrame(data=np.append(x_train,np.asmatrix(y_train),axis = 1))\n",
    "corrMatrix = df.corr()\n",
    "#plt.figure(figsize=(35, 35))\n",
    "#sn.heatmap(corrMatrix, annot=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "def get_valid_indices(original_mat, new_mat):\n",
    "    valid_indices=[]\n",
    "    array_to_index = new_mat[0,:]\n",
    "    array_to_parse = original_mat[0,:]\n",
    "    for elem in array_to_index:\n",
    "        var = np.where(array_to_parse==elem)\n",
    "        valid_indices.append(var[0][0])\n",
    "    #print(valid_indices)\n",
    "    return valid_indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "# feature selection with various statistics...\n",
    "x_train, y_train, y_train_task3 = X_train, np.array(train_labels_1), np.array(train_labels_3)\n",
    "#print(x_train.shape)\n",
    "#correlations = feature_selection.chi2(x_train, y_train)\n",
    "valid_indices = []\n",
    "for y_considered in y_train.T:\n",
    "#y_considered = y_train[:,0]\n",
    "    x_selected_f_reg = feature_selection.SelectKBest(score_func=feature_selection.f_regression, k=20).fit_transform(x_train,y_considered)\n",
    "\n",
    "    x_selected_anova =feature_selection.SelectKBest(score_func=feature_selection.f_classif, k=29).fit_transform(x_train,y_considered)\n",
    "\n",
    "    x_selected_false_pos = feature_selection.SelectFpr(feature_selection.f_regression, alpha=0.001).fit_transform(x_train, y_considered)\n",
    "\n",
    "    indices = get_valid_indices(x_train,x_selected_anova) # current indices of selected feature\n",
    "    valid_indices.append(indices)\n",
    "\n",
    "valid_indices_task3 = []\n",
    "for y_considered in y_train_task3.T:\n",
    "\n",
    "    x_selected_f_reg = feature_selection.SelectKBest(score_func=feature_selection.f_regression, k=20).fit_transform(x_train,y_considered)\n",
    "\n",
    "    x_selected_anova =feature_selection.SelectKBest(score_func=feature_selection.f_classif, k=29).fit_transform(x_train,y_considered)\n",
    "\n",
    "    x_selected_false_pos = feature_selection.SelectFpr(feature_selection.f_regression, alpha=0.001).fit_transform(x_train, y_considered)\n",
    "\n",
    "    indices = get_valid_indices(x_train,x_selected_anova) # current indices of selected feature\n",
    "    valid_indices_task3.append(indices)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sub Task 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if Svm:\n",
    "    clf_1 = {}\n",
    "    #output_1 = np.empty(tot_medical_test)\n",
    "    clf_1[0] = svm.SVC(probability=True)\n",
    "    clf_1[0].fit(x_train, y_train[:,0])\n",
    "    res = clf_1[0].predict_proba(X_test)\n",
    "    print(res.shape)\n",
    "    output_1 = np.asmatrix(res[:,1]).transpose()\n",
    "    print('output_1 dimentions: ', output_1.shape)\n",
    "    for medical_test in range(1,tot_medical_test):\n",
    "        print('medical test: ', medical_test)\n",
    "        start = time.time()\n",
    "        clf_1[medical_test] = svm.SVC(probability=True)\n",
    "        clf_1[medical_test].fit(x_train, y_train[:,medical_test])\n",
    "        res = clf_1[medical_test].predict_proba(X_test)\n",
    "        print(res.shape)\n",
    "        print('output_1 dimentions: ', output_1.shape)\n",
    "        output_1 = np.append(output_1, np.asmatrix(res[:,1]).transpose(), axis = 1)\n",
    "        print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mlp_classification(X_train, X_test, y_train):\n",
    "    clf = MLPClassifier(random_state=1, max_iter=1000).fit(X_train, y_train)\n",
    "    outupt = clf.predict_proba(X_test)\n",
    "    scores = clf.score(X_train,y_train)\n",
    "    return outupt\n",
    "if Mlp:\n",
    "    output_1 = []\n",
    "    for medical_test in range(tot_medical_test):\n",
    "        print('--- test numero: ', medical_test, ' ---')\n",
    "        start = time.time()\n",
    "        output = mlp_classification(X_train,X_test,y_train[:, medical_test])\n",
    "        output_1.append(output[:,1])\n",
    "        print(f\"output current step {output} with dimension {np.shape(output_1)}\")\n",
    "        print(\"time for a fitting\", time.time()-start)\n",
    "        output_1 = np.array(output_1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " start with number :  1\n",
      "\n",
      " finished with number :  1\n",
      "\n",
      " start with model:  classific_model_1.joblib\n",
      "\n",
      " finished with number :  1\n",
      "\n",
      " start with model:  classific_model_2.joblib\n",
      "\n",
      " finished with number :  2\n",
      "\n",
      " start with model:  classific_model_3.joblib\n",
      "\n",
      " finished with number :  3\n",
      "\n",
      " start with model:  classific_model_4.joblib\n",
      "\n",
      " finished with number :  4\n",
      "\n",
      " start with model:  classific_model_5.joblib\n",
      "\n",
      " finished with number :  5\n",
      "\n",
      " start with model:  classific_model_6.joblib\n",
      "\n",
      " finished with number :  6\n",
      "\n",
      " start with model:  classific_model_7.joblib\n",
      "\n",
      " finished with number :  7\n",
      "\n",
      " start with model:  classific_model_8.joblib\n",
      "\n",
      " finished with number :  8\n",
      "\n",
      " start with model:  classific_model_9.joblib\n",
      "\n",
      " finished with number :  9\n",
      "\n",
      " start with model:  classific_model_10.joblib\n",
      "\n",
      " finished with number :  10\n",
      "scores for training :  [0.8892866543827322, 0.943564095814688, 0.8101605685706765, 0.8116346406949198, 0.8025796262174256, 0.8468017899447223, 0.9347196630692287, 0.8503290339563043, 0.9694130034219531, 0.9691497762569097, 0.9427217688865491]\n"
     ]
    }
   ],
   "source": [
    "if GradientBoostingClassification:\n",
    "\n",
    "    score_vec = []\n",
    "    # first prediction\n",
    "    print(\"\\n start with number : \",1)\n",
    "    clf = HistGradientBoostingClassifier(loss = 'binary_crossentropy', max_iter=10**10, l2_regularization=1, verbose=0, warm_start=True, tol=10**-10, learning_rate=0.001).fit(X_train[:,valid_indices[0]], y_train[:,0])\n",
    "    dump(clf, 'classific_model_0.joblib')\n",
    "    print(\"\\n finished with number : \",1)\n",
    "    res = clf.predict_proba(X_test[:,valid_indices[0]])\n",
    "    scores = clf.score(X_train[:,valid_indices[0]], y_train[:,0])\n",
    "    score_vec.append(scores)\n",
    "    output_1 = np.asmatrix(res[:,1]).transpose()\n",
    "\n",
    "    for medical_test in range(1,tot_medical_test):\n",
    "\n",
    "        modelname = \"classific_model_\" + str(medical_test) + \".joblib\"\n",
    "        print(\"\\n start with model: \", modelname)\n",
    "\n",
    "        clf = HistGradientBoostingClassifier(loss = 'binary_crossentropy', max_iter=10**10, l2_regularization=1, verbose=0, warm_start=True, tol=10**-10, learning_rate=0.001).fit(X_train[:,valid_indices[medical_test]], y_train[:,medical_test])\n",
    "\n",
    "        dump(clf, modelname)\n",
    "\n",
    "        res = clf.predict_proba(X_test[:,valid_indices[medical_test]])\n",
    "        scores = clf.score(X_train[:,valid_indices[medical_test]], y_train[:,medical_test])\n",
    "\n",
    "        score_vec.append(scores)\n",
    "        output_1 = np.append(output_1, np.asmatrix(res[:,1]).transpose(), axis = 1)\n",
    "\n",
    "        print(\"\\n finished with number : \", medical_test)\n",
    "\n",
    "        #print('- output_1 dimentions: ', np.shape(output_1))\n",
    "        #print('processing time:    ',time.time()-start)\n",
    "    output_1 = np.array(output_1)\n",
    "    #print('- output_1 dimentions: ', output_1.shape)\n",
    "    print('scores for training : ', score_vec)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SUB-TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if normalridge:\n",
    "    vital_signs = 4\n",
    "    y_3 = np.array(train_labels_3)\n",
    "    pred_label_vec = []\n",
    "    for sign in range(vital_signs):\n",
    "        lambda_vec = np.array([0.1, 1, 10, 100, 200, 500, 1000])  #### lambda vector\n",
    "        number_folds = 10  #### Cross-validation folds\n",
    "\n",
    "        # 1. Iteratively split the date in K = 10 folds\n",
    "        # K-fold cross-validation through sklearn\n",
    "        kfolds_class = KFold(number_folds, shuffle=True)\n",
    "        RMSE_4_lambda = []\n",
    "\n",
    "        # 2. Ridge regression with iteratively different lambdas\n",
    "        for lbd in lambda_vec:\n",
    "            RMSE_list = []\n",
    "            regression_class = linear_model.Ridge(alpha=lbd, solver='svd')\n",
    "            for train_index, test_index in kfolds_class.split(X_train):\n",
    "                x_train, x_validation = X_train[train_index], X_train[test_index]\n",
    "                y_tr, y_val = y_3[train_index, sign], y_3[test_index, sign]\n",
    "\n",
    "                regression_class.fit(x_train, y_tr)\n",
    "                pred_label = regression_class.predict(x_validation)\n",
    "\n",
    "                RMSE_list.append(rmse(pred_label, y_val)**0.5)\n",
    "\n",
    "            RMSE_list = np.array(RMSE_list)\n",
    "            RMSE_4_lambda.append(np.average(RMSE_list))\n",
    "\n",
    "        RMSE_4_lambda = np.array(RMSE_4_lambda)\n",
    "\n",
    "        best_idx = np.argmin(RMSE_4_lambda)\n",
    "        best_lambda = lambda_vec[best_idx]\n",
    "\n",
    "        regression_class = linear_model.Ridge(alpha=best_lambda, solver='svd')\n",
    "        regression_class.fit(X_train, y_3[:,sign])\n",
    "        pred_label = regression_class.predict(X_test)\n",
    "        pred_label_vec.append(pred_label)\n",
    "\n",
    "    output_3 = np.array(pred_label_vec).transpose()\n",
    "    print(\"all rmse\", RMSE_4_lambda)\n",
    "    print(\"best lambda\", best_lambda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if kernelridge:\n",
    "    vital_signs = 4\n",
    "    y_3 = np.array(train_labels_3)\n",
    "    pred_label_vec = []\n",
    "    for sign in range(vital_signs):\n",
    "        print('--- vital sign', sign+1, 'out of', vital_signs)\n",
    "        krr = KernelRidge(alpha=1.0, kernel = 'polynomial')\n",
    "        krr.fit(X_train, y_3[:,sign])\n",
    "        pred_label = regression_class.predict(X_test)\n",
    "        pred_label_vec.append(pred_label)\n",
    "\n",
    "    pred_label_vec = np.array(pred_label_vec)\n",
    "    output_3 = np.array(pred_label_vec).transpose()\n",
    "    print(output_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " start with model :  regression_model_0.joblib\n",
      "\n",
      " finished training with number :  0\n",
      "\n",
      " finished prediction :  0\n",
      "\n",
      " finished prediction :  1\n",
      "\n",
      " finished prediction :  2\n",
      "\n",
      " finished prediction :  3\n",
      "\n",
      " scores for each sign [0.4657236733007133, 0.6474615590943178, 0.4159599334998936, 0.6514824627486163]\n"
     ]
    }
   ],
   "source": [
    "if GradientBoostingRegressor:\n",
    "    vital_signs = 4\n",
    "    y_3 = np.array(train_labels_3)\n",
    "    output_3 = []\n",
    "    scores_vec = []\n",
    "    #for sign in range(vital_signs):\n",
    "    for sign in range(0,vital_signs):\n",
    "    # Loop for training all or specific model\n",
    "        modelname = \"regression_model_\" + str(sign) + \".joblib\"\n",
    "        print(\"\\n start with model : \", modelname)\n",
    "\n",
    "        est = HistGradientBoostingRegressor(max_iter=10**10, l2_regularization=1, loss=\"poisson\", warm_start=True, verbose=0, tol=10**-10, learning_rate=0.001).fit(X_train[:,valid_indices_task3[sign]], y_3[:,sign])\n",
    "\n",
    "        dump(est, modelname)\n",
    "        print(\"\\n finished training with number : \", sign)\n",
    "\n",
    "    for sign in range(vital_signs):\n",
    "    # Load model and perform inference\n",
    "        est = load('regression_model_' + str(sign) + '.joblib')\n",
    "        output = est.predict(X_test[:,valid_indices_task3[sign]])\n",
    "        output_3.append(output)\n",
    "        scores = est.score(X_train[:,valid_indices_task3[sign]], y_3[:,sign])\n",
    "        scores_vec.append(scores)\n",
    "        print(\"\\n finished prediction : \", sign)\n",
    "\n",
    "    output_3 = np.array(output_3).transpose()\n",
    "    print(\"\\n scores for each sign\", scores_vec)\n",
    "\n",
    "    # [0.47423892473321194, 0.6447733667120648, 0.3979460554973725, 0.643170029603557]\n",
    "    #  [0.45289998264792863, 0.6474615590943178, 0.38271032990489495, 0.6514824627486163]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "output_tot = pid\n",
    "#print(header.shape)\n",
    "output_tot = np.append(output_tot, output_1, axis = 1)\n",
    "output_tot = np.append(output_tot, output_3, axis = 1)\n",
    "#output_tot = np.concatenate((header, output_tot), axis=0)\n",
    "df = pd.DataFrame(output_tot)\n",
    "df.to_csv('new_submission.zip', index=False, float_format='%.6f', compression='zip',  header=header)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}